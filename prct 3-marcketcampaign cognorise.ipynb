{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1aZuPKYqFqhx2Xuvn3x_hBQyLD_2XP2NJ","authorship_tag":"ABX9TyMbQ5/6t3L3ApAsDi/TT5Ng"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#Importing the Libraries\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.preprocessing import LabelEncoder,StandardScaler\n","from sklearn.decomposition import PCA\n","from yellowbrick.cluster import KElbowVisualizer\n","from sklearn.cluster import KMeans\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file"],"metadata":{"id":"M-GLoAoT1Q69"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUq0V-Jj07Lw"},"outputs":[],"source":["data=pd.read_csv('/content/drive/MyDrive/cognorise/Data analytics/marketing_campaign.csv',sep=\"\\t\")\n","data"]},{"cell_type":"code","source":["# Display the first few rows of the dataset\n","print(data.head())\n","\n","# Display summary statistics of the dataset\n","print(data.describe())\n","\n"],"metadata":{"id":"6swsGh7bMlHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#get the number of rows and columns\n","print(f\"The number of rows : {data.shape[0]} and the number of columns : {data.shape[1]}\")\n","\n","pd.set_option(\"display.max_columns\",None)\n","\n","data.head()"],"metadata":{"id":"qrxv0X-SMl3N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#info on features\n","data.info()\n"],"metadata":{"id":"gp25AgT3Mx6E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to remove the N/A values\n","data_cleaned = data.dropna()\n","print(\"The total number of data-points after removing the rows with missing values are:\", len(data_cleaned))"],"metadata":{"id":"mrb9DiWKM9sI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for missing values\n","print(data.isnull().sum())\n"],"metadata":{"id":"dYMyA-ESOuGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.dtypes"],"metadata":{"id":"qjgPcFsTO-Q7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Total categories in the feature Marital_Status:\\n\", data_cleaned[\"Marital_Status\"].value_counts(), \"\\n\")\n","print(\"Total categories in the feature Education:\\n\", data_cleaned[\"Education\"].value_counts())"],"metadata":{"id":"D4tLqoB5Nb1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.describe()"],"metadata":{"id":"l6aV_CL6Ne_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data[(data[\"Income\"]<600000)]\n","print(\"The total number of data-points after removing the outliers are:\", len(data))"],"metadata":{"id":"tU7sHfLnRqBC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Feature Engineering**\n","\n","Feature Engineering for Enhanced Data Insight\n","\n","To gain deeper insights from our dataset, we will engineer new features based on existing ones. Below are the enhancements we plan to implement:\n","\n","1:Age Calculation: Derive the \"Age\" of a customer from the \"Year_Birth\" to better understand the demographic distribution.\n","\n","2:Total Spending: Introduce a new feature \"Spent\" that aggregates the total expenditures of a customer across all subcategories, providing a holistic view of customer spending behavior.\n","\n","3:Living Situation: Generate a \"Living_With\" feature from \"Marital_Status\" to categorize customers as living with a \"Partner\" or living \"Alone,\" which aids in understanding household dynamics.\n","\n"],"metadata":{"id":"IwxhJZqwSWq4"}},{"cell_type":"code","source":["# Calculate the age\n","data['Age'] = 2021 - data['Year_Birth']\n","\n","#Total spendings on various items\n","data[\"Spent\"] = data[\"MntWines\"]+ data[\"MntFruits\"]+ data[\"MntMeatProducts\"]+ data[\"MntFishProducts\"]+ data[\"MntSweetProducts\"]+ data[\"MntGoldProds\"]\n","\n","#Deriving living situation by marital status\n","data[\"Living_With\"] = data[\"Marital_Status\"].replace({\"Married\":\"Partner\", \"Together\":\"Partner\", \"Absurd\":\"Alone\", \"Widow\":\"Alone\", \"YOLO\":\"Alone\", \"Divorced\":\"Alone\", \"Single\":\"Alone\",})\n","data[\"Living_With\"] = data[\"Living_With\"].replace({\"Alone\": 1, \"Partner\":2})\n","#Feature indicating total children living in the household\n","data[\"Children\"]=data[\"Kidhome\"]+data[\"Teenhome\"]\n","\n"],"metadata":{"id":"7GOkmj_4RvNk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4:Household Children Count: Create a \"Children\" feature to count the total number of children in a household, encompassing both kids and teenagers.\n","\n","5:Household Size: Establish a \"Family_Size\" feature that sums \"Living_With\" and \"Children\" to provide a clearer picture of the household's composition.\n","\n","6:Simplify Education Levels: Simplify the \"Education\" field into three broad categories to streamline analysis and reduce complexity.\n","\n","7:Feature Reduction: Drop redundant features that will not be utilized in the modeling process to streamline the dataset and focus on the most impactful variables.\n","\n","These transformations are designed to enhance our analytical capabilities and improve the effectiveness of our subsequent modeling efforts."],"metadata":{"id":"Sm_wJYV1S18X"}},{"cell_type":"code","source":["#Feature for total members in the householde\n","data[\"Family_Size\"] = data[\"Living_With\"] + data[\"Children\"]\n","\n","#Segmenting education levels in three groups\n","data[\"Education\"]=data[\"Education\"].replace({\"Basic\":\"Undergraduate\",\"2n Cycle\":\"Undergraduate\", \"Graduation\":\"Graduate\", \"Master\":\"Postgraduate\", \"PhD\":\"Postgraduate\"})\n","#For clarity\n","data=data.rename(columns={\"MntWines\": \"Wines\",\"MntFruits\":\"Fruits\",\"MntMeatProducts\":\"Meat\",\"MntFishProducts\":\"Fish\",\"MntSweetProducts\":\"Sweets\",\"MntGoldProds\":\"Gold\"})\n","\n","#Dropping some of the redundant features\n","to_drop = [\"Marital_Status\", \"Dt_Customer\", \"Z_CostContact\", \"Z_Revenue\", \"ID\", \"Year_Birth\"]\n","data = data.drop(to_drop, axis=1)"],"metadata":{"id":"acaCs3cySPs3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.info()"],"metadata":{"id":"alwTmJlrTBUE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[['Age','Living_With', 'Children', 'Family_Size', 'Income', 'Spent', 'Complain']].describe().T"],"metadata":{"id":"TPzCwXCjTKQB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exploratory Data Analysis**\n","\n","**Univariate Analysis**\n","\n","Univariate analysis involves examining the distribution and characteristics of a single variable in a dataset, often through statistical summaries and visualizations like histograms or bar plots. By plotting the frequency of each feature, we can gain insights into the central tendency (like mean or median), dispersion (such as range and variance), and the presence of outliers or skewness in the data. This analysis helps identify patterns, anomalies, or typical values within each feature, which are crucial for understanding the overall data structure and informing further analysis or preprocessing steps."],"metadata":{"id":"8wEwsyb4Ta09"}},{"cell_type":"code","source":["continuous_columns = ['Age', 'Income', 'Spent']\n","categorical_columns = ['Living_With', 'Children', 'Family_Size', 'Complain', 'Education']\n","all_columns = continuous_columns + categorical_columns\n","\n","# Calculate the number of rows needed for two columns\n","num_rows = (len(all_columns) + 1) // 2\n","\n","# Setup the matplotlib figure and axes\n","fig, axes = plt.subplots(nrows=num_rows, ncols=2, figsize=(10, num_rows * 3))\n","axes = axes.flatten()  # Flatten the axes array to make indexing easier\n","\n","# Iterate over the columns and create appropriate plots\n","for i, col in enumerate(all_columns):\n","    if col in continuous_columns:\n","        sns.histplot(data=data, x=col, bins=20, ax=axes[i], kde=True, color='blue')  # Add density curve for continuous data\n","        axes[i].set_title(f'Histogram of {col}')\n","    elif col in categorical_columns:\n","        sns.countplot(data=data, x=col, ax=axes[i], color='blue')\n","        axes[i].set_title(f'Count Plot of {col}')\n","    axes[i].set_xlabel(col)\n","    axes[i].set_ylabel('Frequency')\n","    # If the number of columns is odd, hide the last subplot (if unused)\n","if len(all_columns) % 2 != 0:\n","    axes[-1].set_visible(False)  # Hide the last axis if not needed\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"_wZ_mD3nTWSg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Bivariate Analysis (features correlation)**\n","\n","Before proceed with the Data Preprocessing and the Machine Learning step, let's visualize the some data correlation and trends.\n","\n","Income x Total Spent:"],"metadata":{"id":"tBwhYP1kT3Oa"}},{"cell_type":"code","source":["# Create the scatter plot with a regression line\n","plt.figure(figsize=(8, 4))\n","sns.regplot(x='Income', y='Spent', data=data, scatter_kws={'s': 20}, line_kws={'color': 'red'})\n","\n","plt.title('Scatter Plot of Income vs. Spent with Trend Line')\n","plt.xlabel('Income')\n","plt.ylabel('Spent')\n","plt.show()"],"metadata":{"id":"Yq0J5Vf2TzLt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Income and Spent are two numerical features strongly correlated. There are some outliers, but I believe it is not necessary to drop them or perform an imputation technique."],"metadata":{"id":"fQUmX8rNUKAT"}},{"cell_type":"markdown","source":["**Average Spent x Age groups:**"],"metadata":{"id":"M4kqBNTLUMgI"}},{"cell_type":"code","source":["# Create bins for the 'Age' column\n","bins = np.arange(data['Age'].min(), data['Age'].max() + 10, 10)  # Adjust the bin width as necessary\n","bin_centers = 0.5 * (bins[:-1] + bins[1:])  # Calculate bin centers\n","\n","data['Age_bins'] = pd.cut(data['Age'], bins=bins, include_lowest=True)\n","\n","# Group by the bins and calculate the mean of 'Spent'\n","grouped = data.groupby(pd.cut(data['Age'], bins=bins, labels=bin_centers))['Spent'].mean().reset_index()\n","grouped.columns = ['Age_bins', 'Spent']\n","\n","# Plotting\n","plt.figure(figsize=(10, 4))\n","sns.histplot(data=data, x='Age', bins=bins, color='lightblue', kde=False, stat='count', label='Age Frequency')\n","\n","# Create a secondary y-axis for the line plot\n","ax2 = plt.twinx()\n","lineplot = sns.lineplot(data=grouped, x='Age_bins', y='Spent', ax=ax2, color='darkblue', marker='o', label='Average Spent')\n","\n","# Setting labels and title\n","plt.title('Histogram of Age with Average Spent Overlay')\n","plt.xlabel('Age')\n","ax2.set_ylabel('Average Spent')\n","plt.ylabel('Frequency')\n","\n","# Adding text labels at each point\n","for x, y in zip(grouped['Age_bins'], grouped['Spent']):\n","    ax2.text(x, y, f'{y:.2f}', color='darkblue', ha='right', size=10)\n","\n","# Handling legends\n","handles, labels = [], []\n","for ax in plt.gcf().axes:\n","    for h, l in zip(*ax.get_legend_handles_labels()):\n","        handles.append(h)\n","        labels.append(l)\n","plt.legend(handles, labels, loc='center right')\n","\n","plt.show()"],"metadata":{"id":"X_9xEGx4UFSr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Average Spent x Education and Family_Size**"],"metadata":{"id":"nX8pGpmUUln1"}},{"cell_type":"code","source":["# Set up the matplotlib figure\n","plt.figure(figsize=(14, 6))\n","\n","# First subplot: Average \"Spent\" by \"Education\"\n","plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n","sns.barplot(x='Education', y='Spent', data=data)\n","plt.title('Average Spent by Education')\n","plt.xlabel('Education')\n","plt.ylabel('Average Spent')\n","\n","# Second subplot: Average \"Spent\" by \"Children\"\n","plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n","sns.barplot(x='Family_Size', y='Spent', data=data)\n","plt.title('Average Spent by Family Size')\n","plt.xlabel('Family Size')\n","plt.ylabel('Average Spent')\n","# Adjust layout\n","plt.tight_layout()\n","\n","plt.show()\n"],"metadata":{"id":"S7lrC_VzUjOZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can observe that Postgraduate customers has the highest average Spent. Single (living alone) customers has the highest average Spent."],"metadata":{"id":"u8-5i-U3UkXq"}},{"cell_type":"markdown","source":["**Places of Purchase:**"],"metadata":{"id":"BE8EBrBdU3FH"}},{"cell_type":"code","source":["totals = {\n","    'Purchase Type': ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumDealsPurchases'],\n","    'Total Purchases': [\n","        data['NumWebPurchases'].sum(),\n","        data['NumCatalogPurchases'].sum(),\n","        data['NumStorePurchases'].sum(),\n","        data['NumDealsPurchases'].sum()\n","    ]\n","}\n","\n","df_totals = pd.DataFrame(totals)\n","\n","# Create the bar chart\n","plt.figure(figsize=(8, 3))\n","sns.barplot(x='Purchase Type', y='Total Purchases', data=df_totals)\n","plt.title('Total Purchases by Type')\n","\n","plt.ylabel('Total Number of Purchases')\n","plt.xticks(rotation=45)  # Rotates labels to avoid overlap\n","plt.show()"],"metadata":{"id":"_GW-ty5OVX8k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In-store purchasing is the most frequent type of purchase among all consumers"],"metadata":{"id":"vCOsmcQcVdrT"}},{"cell_type":"markdown","source":["**Analyzing the Campaigns performance:**"],"metadata":{"id":"g0T-xXj4VhDM"}},{"cell_type":"code","source":["# Calculate the count of acceptances for each campaign\n","campaigns = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n","counts = data[campaigns].sum().reset_index()\n","counts.columns = ['Campaign', 'Count of Acceptances']\n","\n","# Create the bar chart\n","plt.figure(figsize=(6, 3))\n","sns.barplot(x='Campaign', y='Count of Acceptances', data=counts)\n","plt.title('Count of Customers Accepting Each Campaign')\n","plt.xlabel('Campaign')\n","plt.ylabel('Count of Acceptances')\n","plt.xticks(rotation=45)\n","plt.show()"],"metadata":{"id":"p_NPAuRnVYgE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The acceptance and purchase rates for the campaigns are low, with less than 10% of total customers participating. However, campaigns 3, 4, and 5 saw higher acceptance rates compared to campaigns 1 and 2, with campaign 2 performing the worst."],"metadata":{"id":"ET_8ZEbIVptr"}},{"cell_type":"markdown","source":["**Data Preprocessing**\n","\n","We have to perform feature engineering to prepare the data for machine learning. The step includes:\n","\n","\n","*   Label encoding the categorical features.\n","\n","*   Scaling the features using the standard scaler.\n","*   Create a subset dataframe for dimensionality reduction using PCA.\n"],"metadata":{"id":"_8_fimaeVsx5"}},{"cell_type":"code","source":["#Get list of categorical variables\n","cat = (data.dtypes == 'object')\n","object_cols = list(cat[cat].index)\n","\n","print(\"Categorical variables in the dataset:\", object_cols)"],"metadata":{"id":"ZaHuQsMRVlOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a dictionary for manual label encoding\n","education_mapping = {\n","    \"Undergraduate\": 0,\n","    \"Graduate\": 1,\n","    \"Postgraduate\": 2\n","}\n","\n","# Apply the mapping to the 'Education' column\n","data['Education'] = data['Education'].map(education_mapping)"],"metadata":{"id":"LtnHgf9gWIRE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Label Encoding the object dtypes.\n","LE=LabelEncoder()\n","for i in object_cols:\n","    data[i]=data[[i]].apply(LE.fit_transform)"],"metadata":{"id":"8bFK9MVpWLn1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data[\"Living_With\"].value_counts())\n","print(data[\"Education\"].value_counts())"],"metadata":{"id":"l9BstJXcWOFJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we have all the data types as numeric."],"metadata":{"id":"Uxt9D-wNWbWb"}},{"cell_type":"code","source":["#Creating a copy of data\n","ds = data.copy()\n","# creating a subset of dataframe by dropping the features on deals accepted and promotions\n","cols_del = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4','AcceptedCmp5', 'Complain', 'Response', 'Age_bins']\n","ds = ds.drop(cols_del, axis=1)\n","#Scaling\n","scaler = StandardScaler()\n","scaler.fit(ds)\n","scaled_ds = pd.DataFrame(scaler.transform(ds),columns= ds.columns)\n","print(\"All features are now scaled\")"],"metadata":{"id":"0Y8cyVR6WXmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaled_ds.head(5)"],"metadata":{"id":"e1KcTXjuWe38"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Dimensionality Reduction**\n","Dimensionality reduction is a critical preprocessing step before employing KMeans clustering for several reasons:\n","\n","***Improving Clustering Performance:** KMeans clustering calculates distances between points to form clusters. In high-dimensional spaces, distance metrics can become less meaningfulâ€”a phenomenon known as the \"curse of dimensionality.\" Reducing the number of dimensions helps mitigate this issue, making the distance metric more reliable and the clustering results more meaningful.\n","\n","***Enhanced Visualization:** Visualization is a powerful\n","tool for interpreting clustering results. However, visualizing high-dimensional data directly is not feasible. Reducing the data to two or three dimensions allows for effective visualization, enabling easier analysis and communication of the clustering outcomes.\n","\n","***Feature Selection and Noise Reduction:** Dimensionality reduction techniques like Principal Component Analysis (PCA) not only reduce the data dimensions but also help in identifying the most significant variables. This can be viewed as a form of feature selection, where only the features that contribute most to the variance in the data are retained. By focusing on these key features, KMeans can produce more distinct and interpretable clusters."],"metadata":{"id":"4BMsMU0rXC7l"}},{"cell_type":"markdown","source":["**Dimensionality reduction with Principal Component Analysis (PCA)**\n","\n","PCA is a statistical technique used in data analysis to emphasize variation and bring out strong patterns in a dataset. PCA helps in identifying correlations and patterns in data that are not easily identified in raw data. The technique transforms the original variables into a new set of variables, which are called principal components. These principal components are orthogonal (meaning they are uncorrelated), and they are ordered so the first few retain most of the variation present in all of the original variables. PCA is widely used for dimensionality reduction in machine learning and data visualization.\n","\n","The following PCA steps efficiently reduces the dimensionality of the data while attempting to retain the most significant variance present in the original dataset, which is often crucial for visualization, noise reduction, and speeding up further analysis.\n","\n","The PCA object is initialized with n_components=3. This specifies that the PCA transformation will reduce the dimensionality of the dataset to three principal components."],"metadata":{"id":"U8GGwCNtXf7H"}},{"cell_type":"code","source":["#Initiating PCA to reduce dimentions to 3\n","pca = PCA(n_components=3)\n","\n","# Fitting the PCA Model:\n","pca.fit(scaled_ds)\n","\n","# Transforming the Data and Creating a DataFrame:\n","PCA_ds = pd.DataFrame(pca.transform(scaled_ds), columns=([\"col1\",\"col2\", \"col3\"]))\n","\n","# Descriptive statistics\n","PCA_ds.describe().T"],"metadata":{"id":"m01G2QTEWkNQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#A 3D Projection Of Data In The Reduced Dimension\n","x =PCA_ds[\"col1\"]\n","y =PCA_ds[\"col2\"]\n","z =PCA_ds[\"col3\"]\n","#To plot\n","fig = plt.figure(figsize=(8,6))\n","ax = fig.add_subplot(111, projection=\"3d\")\n","ax.scatter(x,y,z, c=\"maroon\", marker=\"o\" )\n","ax.set_title(\"A 3D Projection Of Data In The Reduced Dimension\")\n","plt.show()"],"metadata":{"id":"UVenEOe2Xrfr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Clustering - Customer Segmentation**\n","\n","The steps involved in the Clustering are:\n","\n","*Elbow Method to determine the optimum number of clusters\n","\n","*Employ the KMeans (Clustering)\n","\n","*Examining the clusters"],"metadata":{"id":"sEe0BkUVX_cM"}},{"cell_type":"code","source":["# Quick examination of elbow method to find numbers of clusters to make.\n","print('Elbow Method to determine the number of clusters to be formed:')\n","Elbow_M = KElbowVisualizer(KMeans(random_state=42), k=10)\n","Elbow_M.fit(PCA_ds)\n","Elbow_M.show()"],"metadata":{"id":"2OjbiGrkXydG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The visual above indicates that four is the optimum number of clusters for the data."],"metadata":{"id":"7ivz4BPqYRLN"}},{"cell_type":"code","source":["# Initiating the KMeans Clustering model\n","kmeans = KMeans(n_clusters=4, random_state=42)\n","# Fit model and predict clusters\n","cluster_labels = kmeans.fit_predict(PCA_ds)  # Changed variable name from yhat_kmeans to cluster_labels\n","PCA_ds[\"Clusters\"] = cluster_labels\n","# Adding the Clusters feature to the original dataframe.\n","data[\"Clusters\"] = cluster_labels\n","scaled_ds[\"Clusters\"] = cluster_labels"],"metadata":{"id":"lYUs2IMqX9Xr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To examine the clusters, we can plot the PCA_ds data points in a 3-D space."],"metadata":{"id":"XJUzQHmxYW9f"}},{"cell_type":"code","source":["# Create a new figure for the 3D plot\n","fig = plt.figure(figsize=(8, 6))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","# Extract columns for the plot\n","x = PCA_ds['col1']\n","y = PCA_ds['col2']\n","z = PCA_ds['col3']\n","\n","# Get unique clusters\n","clusters = PCA_ds['Clusters'].unique()\n","\n","# Color map for clusters\n","colors = plt.cm.jet(np.linspace(0, 1, len(clusters)))\n","\n","# Plot each cluster\n","for cluster, color in zip(clusters, colors):\n","    ix = PCA_ds['Clusters'] == cluster\n","    ax.scatter(x[ix], y[ix], z[ix], c=[color], label=f'Cluster {cluster}', s=50)\n","\n","ax.set_title('3D Scatter Plot of PCA Results')\n","ax.legend()\n","plt.show()"],"metadata":{"id":"CCyi-cjFYVIn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It seems that the clusters are well defined, segmenting the dataset into four clusters.\n","\n","**Evaluating the Model**\n","\n","As we are working with unsupervised clustering, we do not have labeled feature to directly evaluate or score our model. Instead, the focus is to examine the patterns within the formed clusters and understand their characteristics.\n","\n","To achieve this, we can conduct exploratory data analysis to visualize the data across different clusters and derive insights, enabling us to draw meaningful conclusions about the underlying cluster patterns.\n","\n","Note that we can evaluate the clusters associated with both the scaled and the original data, as they correspond to the same rows.\n","\n"],"metadata":{"id":"jcHdlWLdYhUd"}},{"cell_type":"code","source":["# Create a bar plot for the number of customers in each cluster\n","plt.figure(figsize=(6, 3))\n","sns.countplot(x='Clusters', data=data)\n","plt.title('Number of Customers in Each Cluster')\n","plt.xlabel('Cluster')\n","plt.ylabel('Number of Customers')\n","plt.show()"],"metadata":{"id":"-ZJ2sstDYdWh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The clusters are well distributed among customers."],"metadata":{"id":"_QpbTTRlYssT"}},{"cell_type":"code","source":["#scatter plot\n","plt.figure(figsize=(8, 4))\n","sns.scatterplot(x='Income', y='Spent', hue='Clusters', data=data, palette='viridis', s=20)\n","\n","plt.title('Scatter Plot of Income vs. Spent by Cluster')\n","plt.xlabel('Income')\n","plt.ylabel('Spent')\n","plt.legend(title='Cluster')\n","plt.show()"],"metadata":{"id":"Pv5JQzs3Yqml"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can notice that:\n","\n","  *Clusters 1 and 3 have both low speding and low income.\n","\n","  *Cluster 0 has average spending and average income.\n","\n","  *Cluster 2 has high speding and high income."],"metadata":{"id":"fZ99Gtu1YxgP"}},{"cell_type":"markdown","source":["**Radar Chart**\n","\n","Radar charts, also known as spider charts, effectively visualize multivariate data, facilitating the comparison of features across different groups. However, it's crucial to note that radar charts can be misleading if there is significant variation in the scales of the variables. Therefore, scaling the data appropriately is essential for accurate and meaningful visualizations.\n","\n"],"metadata":{"id":"vH2EHpvvY80_"}},{"cell_type":"code","source":["from math import pi\n","\n","# Group by 'Clusters' and calculate the mean for 'Income', 'Spent', and 'Family_Size'\n","attributes = ['Income', 'Spent', 'Family_Size', 'Education', 'Age']\n","cluster_means = scaled_ds.groupby('Clusters')[attributes].mean().reset_index()\n","\n","# Number of variables we're plotting.\n","num_vars = len(attributes)\n","\n","# Compute angle each bar is centered on:\n","angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n","angles += angles[:1]  # Complete the loop\n","\n","# Create a radar chart for each cluster\n","fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n","\n","for index, row in cluster_means.iterrows():\n","    values = row[attributes].values.flatten().tolist()\n","    values += values[:1]  # Complete the loop\n","    ax.plot(angles, values, linewidth=1, linestyle='solid', label=f'Cluster {int(row[\"Clusters\"])}')\n","    ax.fill(angles, values, alpha=0.1)\n","\n","# Labels for each attribute.\n","ax.set_xticks(angles[:-1])\n","ax.set_xticklabels(attributes)\n","\n","# Add legend and title\n","plt.title('Average Income, Spent, Family Size, and Education by Cluster')\n","plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n","plt.show()\n"],"metadata":{"id":"9aSnMCI0Yv2J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can make the following observations:\n","\n","**Cluster 0:** This cluster represents a middle group with moderate income and spending, and average family size. It contains the oldest demographic among all clusters.\n","\n","**Cluster 1:** This group exhibits low income and low spending levels, yet it has the largest family size. It predominantly consists of older customers.\n","\n","**Cluster 2:** This group boasts the highest income and spending, typically lives alone, and compromises the midle age group (40-50 years old)\n","\n","**Cluster 3:** Characterized by the lowest income and spending of all groups, this cluster has an average family size and is primarily made up of the youngest customers."],"metadata":{"id":"wTLb8ymRZOFP"}},{"cell_type":"code","source":["attributes = ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumDealsPurchases']\n","cluster_means = scaled_ds.groupby('Clusters')[attributes].mean().reset_index()\n","\n","# Number of variables we're plotting.\n","num_vars = len(attributes)\n","\n","# Compute angle each bar is centered on:\n","angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n","angles += angles[:1]  # Complete the loop\n","\n","# Create a radar chart for each cluster\n","fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n","\n","for index, row in cluster_means.iterrows():\n","    values = row[attributes].values.flatten().tolist()\n","    values += values[:1]  # Complete the loop\n","    ax.plot(angles, values, linewidth=1, linestyle='solid', label=f'Cluster {int(row[\"Clusters\"])}')\n","    ax.fill(angles, values, alpha=0.1)\n","\n","# Labels for each attribute.\n","ax.set_xticks(angles[:-1])\n","ax.set_xticklabels(attributes)\n","\n","# Add legend and title\n","plt.title('Average Income, Spent, Family Size, and Education by Cluster')\n","plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n","plt.show()"],"metadata":{"id":"mFqxhNUpZLQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Group by 'Clusters' and calculate the mean for ...\n","attributes = ['Wines', 'Fruits', 'Meat', 'Fish', 'Sweets', 'Gold']\n","cluster_means = scaled_ds.groupby('Clusters')[attributes].mean().reset_index()\n","\n","# Number of variables we're plotting.\n","num_vars = len(attributes)\n","\n","# Compute angle each bar is centered on:\n","angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n","angles += angles[:1]  # Complete the loop\n","\n","# Create a radar chart for each cluster\n","fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n","\n","for index, row in cluster_means.iterrows():\n","    values = row[attributes].values.flatten().tolist()\n","    values += values[:1]  # Complete the loop\n","    ax.plot(angles, values, linewidth=1, linestyle='solid', label=f'Cluster {int(row[\"Clusters\"])}')\n","    ax.fill(angles, values, alpha=0.1)\n","\n","# Labels for each attribute.\n","ax.set_xticks(angles[:-1])\n","ax.set_xticklabels(attributes)\n","\n","# Add legend and title\n","plt.title('Radar Chart: Average Spending with Wines, Fruits, Meat, Fish, Sweets, and Gold by Cluster')\n","plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n","plt.show()\n"],"metadata":{"id":"z1wK2AkeZ3E2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exploring the campaings performance**\n","\n","It is worth to recall that the campaings attributes were not used to cluster the customers, but its analysis can also give some insights regarding how was the campaings performance in each group (cluster)."],"metadata":{"id":"UTvfwiJJZ_js"}},{"cell_type":"code","source":["# Group by 'Clusters' and calculate the mean for ...\n","attributes = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n","cluster_means = data.groupby('Clusters')[attributes].sum().reset_index()\n","\n","# Number of variables we're plotting.\n","num_vars = len(attributes)\n","\n","# Compute angle each bar is centered on:\n","angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n","angles += angles[:1]  # Complete the loop\n","\n","# Create a radar chart for each cluster\n","fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n","\n","for index, row in cluster_means.iterrows():\n","    values = row[attributes].values.flatten().tolist()\n","    values += values[:1]  # Complete the loop\n","    ax.plot(angles, values, linewidth=1, linestyle='solid', label=f'Cluster {int(row[\"Clusters\"])}')\n","    ax.fill(angles, values, alpha=0.1)\n","\n","# Labels for each attribute.\n","ax.set_xticks(angles[:-1])\n","ax.set_xticklabels(attributes)\n","\n","# Add legend and title\n","plt.title('Radar Chart: Total Accpeted Promotions by Campaing for each Cluster')\n","plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n","plt.show()\n"],"metadata":{"id":"JGtsGGqOZ9aQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Conclusion**\n","\n","In this project, we have meticulously analyzed a Customer Dataset through the creation of customer segmentation. Our workflow encompassed data cleaning, extensive feature engineering, outlier management, and exploratory data analysis enhanced by visualizations. We then proceeded with data preprocessing, which included label encoding for categorical features, scaling of all features, and dimensionality reduction via PCA.\n","\n","During the Machine Learning phase, we utilized the Elbow method to determine the optimal number of clusters (K) for the KMeans algorithm. After fitting the model, we conducted a thorough analysis of the resulting customer segments to extract distinct profiles for each cluster, revealing insightful patterns in consumer behavior.\n","\n","\n"],"metadata":{"id":"qAD5TNaVaPWn"}},{"cell_type":"markdown","source":["Insights on Cluster Profiles:\n","\n","***Cluster 0:**\n","\n","*Exhibits high to average income and spending.\n","*Typically consists of families with 3 to 4 members.\n","*Represents the oldest age demographic.\n","*Shows significant expenditure on wines and gold.\n","*Purchases are well-distributed across catalog, store, and web channels.\n","*Predominantly responds to campaign 4.\n","\n","***Cluster 1:**\n","\n","*Characterized by low income and spending.\n","*Contains the largest families, with 4 to 5 members.\n","*Generally older in age.\n","*Purchases primarily through deals.\n","*Shows a low rate of campaign acceptance, with campaign 3 being the most accepted.\n","\n","***Cluster 2:**\n","\n","*Features the highest income and spending.\n","*Primarily consists of singles.\n","*Age range predominantly between 40 and 50 years.\n","*Favors purchases from catalogs.\n","*Shows a high acceptance rate for campaigns 1 and 5."],"metadata":{"id":"GBbZwo9jaain"}},{"cell_type":"markdown","source":["Cluster 3:\n","\n","Noted for the lowest income and spending.\n","Comprises families of 2 or 3 members.\n","Includes the youngest demographic.\n","Mostly undergraduate or graduate education levels.\n","Frequently purchases through deals.\n","Exhibits the lowest campaign acceptance, with campaign 3 being notably accepted."],"metadata":{"id":"zo9XguCka73P"}},{"cell_type":"markdown","source":["**Future Recommendations**\n","\n","To enhance the robustness and applicability of our findings, future work could explore the integration of additional predictive modeling techniques and the application of advanced algorithms for dynamic segmentation. Employing time series analysis to track changes in customer behavior over time could provide deeper insights into trends and lifecycle patterns. Additionally, conducting A/B testing on selected campaigns within identified segments may offer concrete data on the effectiveness of targeted marketing strategies, thereby maximizing customer engagement and optimizing marketing spend.\n","\n","This thoughtful approach not only underscores the detailed analysis undertaken but also highlights potential avenues for enriching the insights gleaned from customer data."],"metadata":{"id":"XClHJZC7a5FJ"}},{"cell_type":"code","source":["# prompt: save this code to local directory\n","\n","# Save the DataFrame to a CSV file\n","df.to_csv('marcketcampaign.csv', index=False)\n","\n","# Download the CSV file\n","from google.colab import files\n","files.download('marcketcampaign.csv')"],"metadata":{"id":"l8dFtMwJaKKB"},"execution_count":null,"outputs":[]}]}